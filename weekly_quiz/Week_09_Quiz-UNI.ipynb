{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Week 9 & 10 Quiz\n",
    "\n",
    "## [Name] - [UNI]\n",
    "\n",
    "### Due Mon. Dec 2nd, 11:59pm\n",
    "\n",
    "This quiz has two sections. In section 1, we'll practice scaling data and using PCA for dimensionality reduction.\n",
    "In section 2, we focus on NLP and TF-IDF methods.  \n",
    "\n",
    "### Instructions\n",
    "\n",
    "Replace the Name and UNI in cell above and the notebook filename\n",
    "\n",
    "Replace all '____' below using the instructions provided.\n",
    "\n",
    "When completed, \n",
    " 1. make sure you've replaced Name and UNI in the first cell and filename\n",
    " 2. Kernel -> Restart & Run All to run all cells in order \n",
    " 3. Print Preview -> Print (Landscape Layout) -> Save to pdf \n",
    " 4. post pdf to GradeScope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Standard Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy, pandas, matplotlib.pyplot and seaborn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set matplotlib to display inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import load_breast_cancer from sklearn.datasets\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the breast cancer dataset using the load_breast_cancer() function.\n",
    "# Store in the variable 'cancer'.\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# Create a new dataframe X with values from cancer.data (which is stored as a numpy array)\n",
    "#    and with columns named using cancer.feature_names (also a numpy array)\n",
    "X = pd.DataFrame(cancer.data,columns=cancer.feature_names)\n",
    "\n",
    "# For this quiz, only keep the first 10 features/columns\n",
    "# Store the result back into X\n",
    "X = X.iloc[:,:10]\n",
    "\n",
    "# Assert that the shape of the dataframe is (569,10): 569 rows, 10 columns\n",
    "assert X.shape == (569,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution of features in this dataset vary quite a bit, affecting PCA performance.\n",
    "# To get a sense of the difference, display the mean and standard deviation of each feature.\n",
    "# Use the .agg() function, which takes a list of strings describing the functions to apply.\n",
    "# Call .agg() on X\n",
    "#   with the function names 'mean' and 'std' \n",
    "#   transpose the dataframe using .T or .transpose()\n",
    "#   and round to a precision of 2\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Standardize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data to mean 0, standard deviation of 1 using sklearn StandardScaler\n",
    "\n",
    "#Import StandardScaler from sklearn\n",
    "____\n",
    "\n",
    "# To standardize X use StandardScaler with default settings\n",
    "#  do a fit_transform() on X\n",
    "#  store in X_zscore\n",
    "X_zscore = ____\n",
    "\n",
    "# Add feature names by creating a new DataFrame\n",
    "#  containing X_zscore\n",
    "#  with the same column names as the original dataframe X\n",
    "#  store back into X_zscore\n",
    "X_zscore = ____\n",
    "\n",
    "# assert that the mean is near 0 and standard deviation is near 1 for all standardized features\n",
    "assert X_zscore.mean().round(2).eq(0).all() and X_zscore.std().round(2).eq(1).all()\n",
    "\n",
    "# To visually confirm that all features have been standardized:\n",
    "# Call .agg() on X_zscore\n",
    "#   with the function names 'mean' and 'std' \n",
    "#   transpose the dataframe using .T or .transpose()\n",
    "#   and round to a precision of 2\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Variance Described by PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA from sklearn.\n",
    "____\n",
    "\n",
    "# Fit a PCA model to X_zscore using PCA() with default parameters\n",
    "#   and store in pca\n",
    "pca = ____\n",
    "\n",
    "# Create a new DataFrame with 2 columns:\n",
    "#   \"component\" with values 0 to the number of components in pca\n",
    "#   \"cumulative explained variance\" with the .cumsum() of the explained_variance_ratio_ in pca\n",
    "#   store in df_var\n",
    "df_var = ____\n",
    "\n",
    "# Use sns.pointplot() to plot the data from df_var with\n",
    "#   \"component\" on the x-axis\n",
    "#   \"cumulative explained variance\" on the y-axis\n",
    "____\n",
    "\n",
    "# Note that over 55% of the variance is explained by the first component\n",
    "# Over 80% by the first 2 components\n",
    "# Over 90% by the first 4 components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Dataset using First 2 Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform X_zscore using a new PCA model with n_components=2\n",
    "# Store the transformed dataset in X_pca\n",
    "X_pca = ____\n",
    "\n",
    "# Add feature names by creating a new DataFrame\n",
    "#  containing X_pca\n",
    "#  with the column names ['component0','component1']\n",
    "#  store back into X_pca\n",
    "X_pca = ____\n",
    "\n",
    "# Assert that the pca representation has the same number of rows (569) but now 2 columns\n",
    "assert X_pca.shape == (569,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Reduced Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using seaborn, create a scatterplot of the data in X_pca\n",
    "#  with component0 on the x-axis\n",
    "#  and component1 on the y-axis\n",
    "#  Color the points by their class assignment by setting hue=cancer.target\n",
    "#  Capture the returned axis in ax\n",
    "ax = ____\n",
    "\n",
    "# Set the title to 'First 2 Components Colored by Class' using ax\n",
    "____\n",
    "\n",
    "# Note that we haven't used the cancer.target information to generate the pca representation.\n",
    "# We're coloring by cancer.target here to demonstrate that under this transformation\n",
    "#   a linear model will do a decent job of separating the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import fetch_20newsgroups from sklearn.datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load the dataset using fetch_20newsgroups().\n",
    "#   Only fetch the two categories of interest using categories=['sci.space','rec.autos']\n",
    "# Store in the result into newsgroups\n",
    "newsgroups = fetch_20newsgroups(categories=['sci.space','rec.autos'])\n",
    "\n",
    "# Store the newsgroups.data as docs, newsgroups.target as y and newsgroups.target_names as y_names\n",
    "docs = newsgroups.data\n",
    "y = newsgroups.target\n",
    "y_names = newsgroups.target_names\n",
    "\n",
    "# Print the number of observations by printing the length of docs\n",
    "#  You should get 1187\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the text of the first document in docs.\n",
    "# Note: try printing both with and without the print() statement\n",
    "#  with the print statement, linebreaks are parsed,\n",
    "#  without, linebreaks are printed as excape characters\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the target value of the first document in y.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the target_name of the first document using y_names and y.\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CountVectorizer to Convert To TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer from sklearn\n",
    "____\n",
    "\n",
    "# Initialize a CountVectorizer object. It should\n",
    "#   lowercase all text, \n",
    "#   include both unigrams and bigrams: ngram_range=(1,2)\n",
    "#   exclude terms that occur in fewer than 10 documents: min_df=10\n",
    "#   exclude terms that occur in more than 95% of documents: max_df=.95\n",
    "# Store as cvect\n",
    "____\n",
    "\n",
    "# Fit cvect on docs and transform docs into their term frequency representation.\n",
    "# Store as X_tf\n",
    "____\n",
    "\n",
    "# Print the shape of X_tf. \n",
    "# The number of rows should match the number of documents above\n",
    "#  and the number of columns should be near 6000\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the last 5 terms in the learned vocabulary in cvect\n",
    "#   using .get_feature_names_out() which returns a list of terms corresponding\n",
    "#   to the order of the columns in X_tf\n",
    "# They should all be related to zoos or zoology\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The stopwords learned by cvect are stored as a set in cvect.stop_words_\n",
    "# We'd like to print out a small subset of these terms.\n",
    "# One way to get a subset of a set is to treat it as a list.\n",
    "# First, convert the stop_words_ set to a list.\n",
    "# Store as stop_words_list\n",
    "____\n",
    "\n",
    "# Print out the first 5 elements in stop_words_list.\n",
    "# Note that, since a set is unordered, \n",
    "#  there is no meaning to the ordering of these terms and they may vary over runs.\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Mean CV Accuracy Using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cross_val_score from sklearn\n",
    "____\n",
    "\n",
    "# Import RandomForestClassifier from sklearn\n",
    "____\n",
    "\n",
    "# Get a set of 5-fold CV scores using\n",
    "#  a RandomForestClassifier \n",
    "#   with 50 trees \n",
    "#   and n_jobs=-1 all other settings default\n",
    "#   and the full dataset X_tf and y\n",
    "# Store as cv_scores\n",
    "____\n",
    "\n",
    "# Print the mean of these cv_scores rounded to a precision of 2.\n",
    "#  The mean accuracy should be above .9\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Find Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer stores the feature names (terms in the vocabulary) in two ways:\n",
    "#  1. as a dictionary of term:colum_index pairs, accessed via cvect.vocabulary_\n",
    "#  2. as a list of terms, in column index order, accessed via cvect.get_feature_names_out()\n",
    "#\n",
    "# We can get the indices of the most important features by training a new RandomForestClassifier on X_tf,y\n",
    "#  and accessing .feature_importances_\n",
    "#\n",
    "# Using some combination of the above data-structures, \n",
    "#  print out the top 10 terms in the vocabulary\n",
    "#  ranked by the feature importances learned by a RandomForestClassifier with 50 trees\n",
    "# \n",
    "# The terms you find will likely not be surprising given the document categories.\n",
    "____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eods-f22",
   "language": "python",
   "name": "eods-f22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
