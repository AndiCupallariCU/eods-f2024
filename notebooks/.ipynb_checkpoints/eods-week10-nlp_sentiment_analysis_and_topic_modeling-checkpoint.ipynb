{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Elements Of Data Science - F2023</center>\n",
    "# <center>Week 10: NLP, Sentiment Analysis and Topic Modeling<center>\n",
    "### <center>11/27/2023</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TODOs\n",
    "\n",
    "- Readings:\n",
    "  -  PML Chapter 11: Working with Unlabeled Data - Clustering Analysis, Sections 11.1 and 11.2\n",
    "  - [Optional] [PDSH 5.11 k-Means](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)\n",
    "  - [Optional] [Data Science From Scratch Chap 22: Recommender Systems](https://ezproxy.cul.columbia.edu/login?qurl=https%3a%2f%2fsearch.ebscohost.com%2flogin.aspx%3fdirect%3dtrue%26db%3dnlebk%26AN%3d979529%26site%3dehost-live%26scope%3dsite%26ebv%3DEB%26ppid%3Dpp_267)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- Quiz 10, Due **Mon Dec 4 15th, 11:59pm ET**\n",
    "- HW3, Due **Fri Dec 1st 11:59pm**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today\n",
    "\n",
    "- **Pipelines**\n",
    "- **NLP**\n",
    "- **Sentiment Analysis**\n",
    "- **Topic Modeling**\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# <center>Questions?</center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipelines in sklearn\n",
    "\n",
    "- Pipelines are wrappers used to string together transformers and estimators\n",
    " - sequentially apply a series of transforms, eg, `.fit_transform()` and `.transform()`\n",
    " - followed by a prediction, eg. `.fit()` and `.predict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipelines in sklearn\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align=\"center\"><img src=\"images/pipelines.png\" width=\"800px\"></div>\n",
    "\n",
    "<font size=6>From PML</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Binary Classification With All Numeric Features Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Example from PML - scaling > feature extraction > classification\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "bc = load_breast_cancer()\n",
    "X_bc,y_bc = bc['data'],bc['target']\n",
    "X_bc_train,X_bc_test,y_bc_train,y_bc_test = train_test_split(X_bc,\n",
    "                                                             y_bc,\n",
    "                                                             test_size=0.3,\n",
    "                                                             stratify=y_bc,\n",
    "                                                             random_state=123)\n",
    "\n",
    "# print without scientific notation\n",
    "numpy.set_printoptions(suppress = True)\n",
    "\n",
    "print(\"training set has rows: {} columns: {}\".format(*X_bc_train.shape))\n",
    "\n",
    "# all real valued features\n",
    "print('Feature names: ',bc.feature_names[:3], ' ...')\n",
    "print('Corresponding Feature values:', X_bc_train[:1,:3][0].round(2), ' ...')\n",
    "print('Target names: ', bc.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipelines in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Pipeline: list of (name,object) pairs\n",
    "pipe1 = Pipeline([('scale',StandardScaler()),                   # scale\n",
    "                  ('pca',PCA(n_components=15)),                  # reduce dimensions\n",
    "                  ('lr',LogisticRegression(solver='saga',\n",
    "                                           max_iter=1000,\n",
    "                                           random_state=12)),  # classifier\n",
    "                 ])\n",
    "\n",
    "pipe1.fit(X_bc_train,y_bc_train)\n",
    "\n",
    "print(f'train set accuracy: {pipe1.score(X_bc_train,y_bc_train).round(3)}')\n",
    "print(f'test set accuracy : {pipe1.score(X_bc_test,y_bc_test).round(3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# access pipeline components by name like a dictionary\n",
    "pipe1['lr'].coef_.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipe1['pca'].components_[0].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipelines in sklearn: GridSearch with Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- specify grid points using 'step name' + '__' (double-underscore) + 'argument'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.exceptions import ConvergenceWarning # needed to supress warnings\n",
    "from sklearn.utils import parallel_backend        # needed to supress warnings\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# separate step-names and argument-names with double-underscore '__'\n",
    "params1 = {'pca__n_components':[2,10,15,20],\n",
    "           'lr__penalty':['none','l1','l2'],\n",
    "           'lr__C':[0,.01,1,10,100]}\n",
    "\n",
    "with parallel_backend(\"multiprocessing\"):         # needed to supress warnings\n",
    "    with warnings.catch_warnings():                 # needed to supress warnings\n",
    "        warnings.filterwarnings(\"ignore\")             # needed to supress warnings\n",
    "    \n",
    "gscv = GridSearchCV(pipe1, params1, cv=3, n_jobs=-1).fit(X_bc_train,y_bc_train)\n",
    "\n",
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "score = gscv.score(X_bc_test,y_bc_test)\n",
    "print(f'test set accuracy: {score:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Displaying Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gscv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(gscv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Displaying Pipelines Cont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gscv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(gscv.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipelines in sklearn with `make_pipeline`\n",
    "\n",
    "- shorthand for Pipeline\n",
    "- step names are lowercase of class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# make_pipeline: arguments in order of how they should be applied\n",
    "pipe2 = make_pipeline(StandardScaler(),                    # center and scale data\n",
    "                      PCA(n_components=2),                 # extract 2 dimensions\n",
    "                      LogisticRegression(random_state=123) # classify using logistic regression\n",
    "                     )\n",
    "pipe2.fit(X_bc_train,y_bc_train) \n",
    "\n",
    "pipe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipe2['logisticregression'].coef_.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ColumnTransformer\n",
    "\n",
    "- Transform sets of columns differently as part of a pipeline\n",
    "- For example: makes it possible to transform categorical and numeric differently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Binary Classification With Mixed Features, Missing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# from https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py\n",
    "titanic_url = ('https://raw.githubusercontent.com/amueller/'\n",
    "               'scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv')\n",
    "df_titanic = pd.read_csv(titanic_url)[['age','fare','embarked','sex','pclass','survived']]\n",
    "# Numeric Features:\n",
    "# - age: float.\n",
    "# - fare: float.\n",
    "# Categorical Features:\n",
    "# - embarked: categories encoded as strings {'C', 'S', 'Q'}.\n",
    "# - sex: categories encoded as strings {'female', 'male'}.\n",
    "# - pclass: ordinal integers {1, 2, 3}.\n",
    "df_titanic.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ColumnTransformer Cont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# specify columns subset\n",
    "numeric_features = ['age', 'fare']\n",
    "# specify pipeline to apply to those columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')), # fill missing values with median\n",
    "    ('scaler', StandardScaler())])                 # scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), # fill missing value with 'missing'\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])                   # one hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# combine column pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', numeric_transformer, numeric_features),\n",
    "                  ('cat', categorical_transformer, categorical_features)\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# add a final prediction step\n",
    "pipe3 = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', LogisticRegression(solver='lbfgs', random_state=42))\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ColumnTransformer Cont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipe3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ColumnTransformer Cont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_titanic = df_titanic.drop('survived', axis=1)\n",
    "y_titanic = df_titanic['survived']\n",
    "\n",
    "X_titanic_train, X_titanic_test, y_titanic_train, y_titanic_test = train_test_split(X_titanic, \n",
    "                                                                                    y_titanic, \n",
    "                                                                                    test_size=0.2, \n",
    "                                                                                    random_state=142)\n",
    "pipe3.fit(X_titanic_train, y_titanic_train)\n",
    "print(f\"train set score: {pipe3.score(X_titanic_train, y_titanic_train).round(3)}\")\n",
    "print(f\"test set score : {pipe3.score(X_titanic_test, y_titanic_test).round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# grid search deep inside the pipeline\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'classifier__C': [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "gs_pipeline = GridSearchCV(pipe3, param_grid, cv=3)\n",
    "gs_pipeline.fit(X_titanic_train, y_titanic_train)\n",
    "print(f\"best test set score from grid search: {gs_pipeline.score(X_titanic_test, y_titanic_test).round(3)}\")\n",
    "print(f\"best parameter settings: {gs_pipeline.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ColumnTransformer Cont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# <center>Questions re Pipelines?</center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing (NLP)\n",
    "<br>\n",
    "\n",
    "- Analyzing and interacting with natural language\n",
    "- Python Libraries\n",
    "  - **sklearn**\n",
    "  - nltk\n",
    "  - **spaCy**\n",
    "  - gensim\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing (NLP)\n",
    "<br>\n",
    "\n",
    "- Many NLP Tasks\n",
    "\n",
    "  - **sentiment analysis**\n",
    "  - **topic modeling**\n",
    "  - entity detection\n",
    "  - machine translation\n",
    "  - natural language generation\n",
    "  - question answering\n",
    "  - relationship extraction\n",
    "  - automatic summarization\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recall: Python Builtin String Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = \"D.S. is fun!\"\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc.lower(),doc.upper()       # change capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc.split() , doc.split('.')  # split a string into parts (default is whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "' | '.join(['ab','c','d'])      # join items in a list together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'|'.join(doc[:5])             # a string itself is treated like a list of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'  tes t   '.strip()           # remove whitespace from the beginning and end of a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- and many more, see [https://docs.python.org/3.10/library/string.html](https://docs.python.org/3.10/library/string.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: The Corpus\n",
    "<br>\n",
    "\n",
    "- **corpus:** collection of documents\n",
    "  - books\n",
    "  - articles\n",
    "  - reviews\n",
    "  - tweets\n",
    "  - resumes\n",
    "  - sentences?\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Doc Representation\n",
    "<br>\n",
    "\n",
    "- Documents usually represented as strings\n",
    "  - string: a sequence (list) of unicode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sample_doc = \"D.S. is fun!\\nIt's  true.\"\n",
    "print(sample_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'|'.join(sample_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Need to split this up into parts (**tokens**)\n",
    "- Good job for **Regular Expressions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aside: Regular Expressions\n",
    "<br>\n",
    "\n",
    "- Strings that define search patterns over text\n",
    "- Useful for finding/replacing/grouping\n",
    "- python `re` library (others available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(sample_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# Find all of the whitespaces in doc\n",
    "# '\\s+' means \"one or more whitespace characters\"\n",
    "re.findall(r'\\s+',sample_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aside: Regular Expressions\n",
    "\n",
    "Just some of the special character definitions:\n",
    "    \n",
    "- `.` : any single character except newline (r'.' matches 'x')\n",
    "- `*` : match 0 or more repetitions (r'x*' matches 'x','xx','')\n",
    "- `+` : match 1 or more repetitions (r'x+' matches 'x','xx')\n",
    "- `?` : match 0 or 1 repetitions (r'x?' matches 'x' or '')\n",
    "<br>\n",
    "    \n",
    "- `^` : beginning of string (r'^D' matches 'D.S.')\n",
    "- `$` : end of string (r'fun!$' matches 'DS is fun!'`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aside: Regular Expression Cont.\n",
    "<br>\n",
    "\n",
    "- `[]` : a set of characters (^ as first element = not)\n",
    "- `\\s` : whitespace character (Ex: [ \\t\\n\\r\\f\\v])\n",
    "- `\\S` : non-whitespace character (Ex: [^ \\t\\n\\r\\f\\v])\n",
    "- `\\w` : word character (Ex: [a-zA-Z0-9_])\n",
    "- `\\W` : non-word character\n",
    "- `\\b` : boundary between \\w and \\W\n",
    "- and many more!\n",
    "<br>\n",
    "\n",
    "- See [regex101.com](https://regex101.com) for examples and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aside: Regex Python Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "r'\\w*u\\w*' # a string of word characters containing the letter 'u'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "re.findall(r'\\w*u\\w*',sample_doc) # return all substrings that match a pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "re.sub(r'\\w*u\\w*','XXXX',sample_doc) # substitute all substrings that match a pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "re.split(r'\\w*u\\w*',sample_doc) # split substrings on a pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Tokenization\n",
    "\n",
    "- **tokens:** strings that make up a document ('the', 'cat',...)\n",
    "- **tokenization:** convert a document into tokens\n",
    "- **vocabulary:** set of unique tokens (terms) in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# split on whitespace\n",
    "re.split(r'\\s+', sample_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# find tokens of length 2+ word characters\n",
    "re.findall(r'\\b\\w\\w+\\b',sample_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# find tokens of length 2+ non-space characters\n",
    "re.findall(r\"\\b\\S\\S+\\b\", sample_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# example vocabulary\n",
    "set(re.findall(r\"\\b\\S\\S+\\b\", sample_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Tokenization in spaCy\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align=\"center\"><img src=\"images/spacy_tokenization.svg\" width=\"500px\"></align>\n",
    "\n",
    "<font size=5>From [https://spacy.io/usage/linguistic-features](https://spacy.io/usage/linguistic-features)</font>\n",
    "\n",
    "First, the raw text is split on whitespace characters, similar to text.split(' '). Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n",
    "- Does the substring match a tokenizer exception rule? For example, “don’t” does not contain whitespace, but should be split into two tokens, “do” and “n’t”, while “U.K.” should always remain one token.\n",
    "- Can a prefix, suffix or infix be split off? For example punctuation like commas, periods, hyphens or quotes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Other Options for Preprocessing\n",
    "<br>\n",
    "\n",
    "- lowercase\n",
    "- remove special characters\n",
    "- add `<START>`, `<END>` tags\n",
    "\n",
    "- **lemmatization:** perform morphological analysis\n",
    "  - 'studies' becomes 'study'\n",
    "  - 'studying' becomes 'study'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Bag of Words\n",
    "    \n",
    "- **Bag of Words** (BOW) representation: ignore token order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sample_doc.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sorted(re.findall(r'\\b\\S\\S+\\b', sample_doc.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: n-Grams\n",
    "\n",
    "- **Unigram:** single token\n",
    "- **Bigram:** combination of two ordered tokens\n",
    "- **n-Gram:** combination of n ordered tokens\n",
    "- The larger *n* is, the larger the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Bigram example:\n",
    "tokens = '<start> ds is fun ds is great <end>'.split()\n",
    "print(\"bigrams     : \",    [tokens[i]+'_'+tokens[i+1] for i in range(len(tokens)-1)])\n",
    "print(\"bigram vocab: \",set([tokens[i]+'_'+tokens[i+1] for i in range(len(tokens)-1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Trigrams example:\n",
    "tokens = '<start> ds is fun ds is great <end>'.split()\n",
    "['_'.join(tokens[i:i+3]) for i in range(len(tokens)-2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: TF and DF\n",
    "\n",
    "- **Term Frequency:** number of times a term is seen per document\n",
    "- $\\text{tf}(t, d) = \\text{count of term } t \\text{ in document } d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "example_corpus = ['red green blue', 'red blue blue']\n",
    "\n",
    "#Vocabulary\n",
    "example_vocab = sorted(set(' '.join(example_corpus).split()))\n",
    "example_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#TF\n",
    "from collections import Counter\n",
    "example_tf = np.zeros((len(example_corpus),len(example_vocab)))\n",
    "for i,doc in enumerate(example_corpus):\n",
    "    for j,term in enumerate(example_vocab):\n",
    "        example_tf[i,j] = Counter(doc.split())[term]\n",
    "example_tf = pd.DataFrame(example_tf,index=['doc1','doc2'],columns=example_vocab)\n",
    "example_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: TF and DF\n",
    "\n",
    "- **Document Frequency:** number of documents containing each term\n",
    "$\\text{df}(t) = \\text{count of documents containing term } t$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "example_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#DF\n",
    "example_df = example_tf.astype(bool).sum(axis=0) # how many documents contain each term (column) \n",
    "example_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Stopwords\n",
    "\n",
    "- terms that have high (or very low) DF and aren't informative\n",
    "  - common engish terms (ex: a, the, in,...)\n",
    "  - domain specific (ex, in class slides: 'data_science')\n",
    "  - often removed prior to analysis\n",
    "  - in sklearn\n",
    "    - `min_df` : integer > 0 : keep terms that occur in at at least n documents\n",
    "    - `max_df` : float in (0,1] :  keep terms that occur in less than max_df% of total documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: CountVectorizer in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "example_corpus = ['blue green red', 'blue green green']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvect = CountVectorizer(lowercase=True,    # default, transform all docs to lowercase\n",
    "                        ngram_range=(1,1), # default, only unigrams\n",
    "                        min_df=1,          # default, keep all terms\n",
    "                        max_df=1.0,        # default, keep all terms\n",
    "                       )\n",
    "X_cv = cvect.fit_transform(example_corpus)\n",
    "X_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cvect.vocabulary_ # learned vocabulary, term:index pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cvect.get_feature_names() # vocabulary, sorted by indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_cv.todense() # term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cvect.inverse_transform(X_cv) # mapping back to terms via vocabulary mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: TfIdf\n",
    "\n",
    "- What if some terms are still uninformative?\n",
    "- Can we downweight terms that occur in many documents?\n",
    "- **Term Frequency * Inverse Document Frequency (tf-idf)**\n",
    "  - $\\text{tf-idf}(t,d) = \\text{tf}(t, d) \\times \\text{idf}(t)$\n",
    "  - $\\text{idf}(t) = \\log \\frac{1+n}{1+\\text{df}(t)} + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvect = TfidfVectorizer(norm='l2') # by default, also doing l2 normalization\n",
    "\n",
    "X_tfidf = tfidfvect.fit_transform(example_corpus)\n",
    "sorted(tfidfvect.vocabulary_.items(),key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_tfidf.todense().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# can also use to get term frequencies by setting use_idf to False and norm to none\n",
    "TfidfVectorizer(use_idf=False, norm=None).fit_transform(example_corpus).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "ngs = fetch_20newsgroups(categories=['rec.sport.baseball','rec.sport.hockey']) # dataset has 20 categories, only get two\n",
    "\n",
    "docs_ngs = ngs['data']                         # get documents (emails)\n",
    "y_ngs = ngs['target']                          # get targets ([0,1])\n",
    "target_names_ngs = ngs['target_names']         # get target names (['rec.sport.baseball','rec.sport.hockey'])\n",
    "\n",
    "print(y_ngs[1], target_names_ngs[y_ngs[1]])    # print target int and target name\n",
    "print('-'*50)                                  # print a string of 50 dashes\n",
    "print(docs_ngs[0].strip()[:600])               # print beginning characters of first doc, after stripping whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Example: Transform Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "docs_ngs_train,docs_ngs_test,y_ngs_train,y_ngs_test = train_test_split(docs_ngs,y_ngs, random_state = 123)\n",
    "\n",
    "vect = TfidfVectorizer(lowercase=True,\n",
    "                       min_df=5,           # occur in at least 5 documents\n",
    "                       max_df=0.8,         # occur in at most 80% of documents\n",
    "                       token_pattern=r'\\b\\S\\S+\\b',  # tokens of at least 2 non-space characters\n",
    "                       ngram_range=(1,1),  # only unigrams\n",
    "                       use_idf=False,      # term frequency counts instead of tf-idf\n",
    "                       norm=None           # do not normalize\n",
    "                      )\n",
    "X_ngs_train = vect.fit_transform(docs_ngs_train)\n",
    "X_ngs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# first few terms in learned vocabulary\n",
    "list(vect.vocabulary_.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# first few terms in learned stopword list\n",
    "list(vect.stop_words_)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# first few terms in BOW representation of first document\n",
    "vect.inverse_transform(X_ngs_train[0])[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Example: Train and Evaluate Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "scores_dummy = cross_val_score(DummyClassifier(strategy='most_frequent'),X_ngs_train,y_ngs_train)\n",
    "scores_lr    = cross_val_score(LogisticRegression(),X_ngs_train,y_ngs_train)\n",
    "\n",
    "print(f'dummy cv accuracy: {scores_dummy.mean().round(2):0.2f} +- {scores_dummy.std().round(2):0.2f}')\n",
    "print(f'lr    cv accuracy: {scores_lr.mean().round(2):0.2f} +- {scores_lr.std().round(2):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Example: Using Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Recall: use Pipeline instead of make_pipeline to add names to the steps\n",
    "#  (name,object) tuple pairs for each step\n",
    "pipe_ngs1 = Pipeline([('vect',TfidfVectorizer(lowercase=True,\n",
    "                                              min_df=5,\n",
    "                                              max_df=0.8,\n",
    "                                              token_pattern=r'\\b\\S\\S+\\b',\n",
    "                                              ngram_range=(1,1),\n",
    "                                              use_idf=False,\n",
    "                                              norm=None )\n",
    "                      ),   \n",
    "                      ('lr',LogisticRegression())\n",
    "                     ])\n",
    "\n",
    "pipe_ngs1.fit(docs_ngs_train,y_ngs_train) # pass in docs, not transformed X\n",
    "\n",
    "score_ngs1 = pipe_ngs1.score(docs_ngs_train,y_ngs_train).round(2)\n",
    "print(f'lr pipeline accuracy on training set: {score_ngs1:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scores_ngs1 = cross_val_score(pipe_ngs1,docs_ngs_train,y_ngs_train) \n",
    "print(f'lr pipeline cv accuracy: {scores_ngs1.mean().round(2):0.2f} +- {scores_ngs1.std().round(2):0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "list(pipe_ngs1['vect'].get_feature_names_out())[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Example: Add Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel,SelectPercentile\n",
    "\n",
    "pipe_ngs2 = Pipeline([('vect',TfidfVectorizer(lowercase=True,\n",
    "                                              min_df=5,\n",
    "                                              max_df=0.8,\n",
    "                                              token_pattern='\\\\b\\\\S\\\\S+\\\\b',\n",
    "                                              ngram_range=(1,1),\n",
    "                                              use_idf=False,\n",
    "                                              norm=None )\n",
    "                      ),   \n",
    "                      ('fs',SelectFromModel(estimator=LogisticRegression(C=1.0,\n",
    "                                                                         penalty='l1',\n",
    "                                                                         solver='liblinear',\n",
    "                                                                         max_iter=1000,\n",
    "                                                                         random_state=123\n",
    "                                                                        ))),\n",
    "                      ('lr',LogisticRegression(max_iter=10000))\n",
    "                     ])\n",
    "\n",
    "pipe_ngs2.fit(docs_ngs_train,y_ngs_train)\n",
    "print(f'pipeline accuracy on training set: {pipe_ngs2.score(docs_ngs_train,y_ngs_train).round(2):0.2f}')\n",
    "\n",
    "scores_ngs2 = cross_val_score(pipe_ngs2,docs_ngs_train,y_ngs_train) \n",
    "print(f'pipeline cv accuracy             : {scores_ngs2.mean().round(2):0.2f} +- {scores_ngs2.std().round(2):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Example: Grid Search with Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# NOTE: this may take a minute or so\n",
    "params_ngs2 = {'vect__use_idf':[True,False],\n",
    "              'vect__ngram_range':[(1,1),(2,2)],\n",
    "              'fs__estimator__C':[10,1000],\n",
    "              'lr__C':[.01,1,100]}\n",
    "\n",
    "gscv_ngs = GridSearchCV(pipe_ngs2, params_ngs2, cv=2, n_jobs=-1).fit(docs_ngs_train,y_ngs_train)\n",
    "\n",
    "print(f'gscv_ngs best parameters  : {gscv_ngs.best_params_}')\n",
    "print(f'gscv_ngs best cv accuracy : {gscv_ngs.best_score_.round(2):0.2f}')\n",
    "print(f'gscv_ngs test set accuracy: {gscv_ngs.score(docs_ngs_test,y_ngs_test).round(2):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sentiment Analysis and sklearn\n",
    "<br>\n",
    "\n",
    "- determine sentiment/opinion from unstructured test\n",
    "- usually positive/negative, but is domain specific\n",
    "- can be treated as a classification task (with a target, using all of the tools we know)\n",
    "- can also be treated as a linguistic task (sentence parsing)\n",
    "<br>\n",
    "\n",
    "- Example: determine sentiment of movie reviews\n",
    "- see [sentiment_analysis_example.ipynb](sentiment_analysis_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling\n",
    "\n",
    "- What topics are our documents composed of?\n",
    "- How much of each topic does each document contain?\n",
    "- Can we represent documents using topic weights? (dimensionality reduction!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What is topic modeling?\n",
    "- How does **Latent Dirichlet Allocation (LDA)** work?\n",
    "- How to train and use LDA with sklearn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Topic Modeling?\n",
    "<br>\n",
    "\n",
    "- **topic:** a collection of related words\n",
    "- A document can be composed of several topics\n",
    "<br>\n",
    "\n",
    "- Given a collection of documents, we can ask:\n",
    "  - **What terms make up each topic?** (per topic term distribution)\n",
    "  - **What topics make up each document?** (per document topic distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling with Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "- Unsupervised method for determining topics and topic assignments\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align=\"center\"><img src=\"images/lda_blei.jpg\" width=\"1100px\"></div>\n",
    "\n",
    "<font size=5>From David Blei</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Two Important Matrices Learned by LDA\n",
    "\n",
    "- the **per topic term distributions** aka $\\varphi$ (phi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "topics = ['topic1','topic2']\n",
    "vocab = ['cat','baseball','play']\n",
    "phi = pd.DataFrame([[0.4,.2,.4],[0.2,.4,.4]],columns=vocab,index=topics)\n",
    "phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the **per document TOPIC distributions** aka $\\theta$ (theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "topics = ['topic1','topic2']\n",
    "docs = ['doc1','doc2']\n",
    "theta = pd.DataFrame([[0.1,.9],[.5,.5]],columns=topics,index=docs)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling: Example\n",
    "\n",
    "- Given the data and the number of topics we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "corpus = ['the dog and cat played tennis',\n",
    "          'tennis and baseball are sports',\n",
    "          'a dog or a cat can be a pet']\n",
    "\n",
    "M = 3 # the number of documents\n",
    "\n",
    "vocab = ['baseball','cat','dog','pet','played','tennis']\n",
    "\n",
    "V = len(vocab) # size of vocabulary\n",
    "\n",
    "K = 2 # our guess about the number of topics\n",
    "\n",
    "print(f'{M = :}\\n{V = :}\\n{K = :}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling: Example\n",
    "\n",
    "- Guessing some **per topic term distributions** ($\\varphi$) given the documents and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# the probability of each term given topic 1 (high for sports terms)\n",
    "topic_1 = [.33,   0,   0,   0, .33, .33]\n",
    "\n",
    "# the probability of each term given topic 2 (high for pet terms)\n",
    "topic_2 = [  0, .25, .25, .25, .25,   0]\n",
    "\n",
    "# per topic term distributions\n",
    "phi = pd.DataFrame([topic_1, topic_2],columns=vocab,\n",
    "                   index=['topic_'+str(x) for x in range(1,K+1)])\n",
    "\n",
    "phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling: Example\n",
    "\n",
    "- Guessing the **per document topic distributions** $\\theta$ given the **topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Given our guess about phi\n",
    "display(phi)\n",
    "# And the corpus\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# generate a guess about per document topic distributions\n",
    "theta = pd.DataFrame([[.50, .50],\n",
    "                      [.99, .01],\n",
    "                      [.01, .99]],\n",
    "                     columns=['topic_'+str(x) for x in range(1,K+1)],\n",
    "                     index=['doc_'+str(x) for x in range(1,M+1)])\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling With LDA\n",
    "\n",
    "- Given\n",
    "  - a set of documents\n",
    "  - a number of topics $K$\n",
    "<br>\n",
    "\n",
    "- Learn\n",
    "  - the **per topic term distributions $\\varphi$ (phi)**, size: $K \\times V$\n",
    "  - the **per document topic distributions $\\theta$ (theta)**, size: $M \\times K$\n",
    "<br>\n",
    "\n",
    "- How to learn $\\varphi$ and $\\theta$:\n",
    "  - Latent Dirichlet Allocation (LDA)\n",
    "  - generative statistical model\n",
    "  - Blei, D., Ng, A., Jordan, M. Latent Dirichlet allocation. J. Mach. Learn. Res. 3 (Jan 2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling With LDA\n",
    "\n",
    "- Uses for $\\varphi$ (phi), the per topic term distributions:\n",
    "  - infering labels for topics\n",
    "  - word clouds\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- Uses for $\\theta$ (theta), the per document topic distributions:\n",
    "  - dimensionality reduction\n",
    "  - clustering\n",
    "  - similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LDA with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# load data from all 20 newsgroups\n",
    "newsgroups = fetch_20newsgroups()\n",
    "ngs_all = newsgroups.data\n",
    "len(ngs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# transform documents using tf-idf\n",
    "tfidf = TfidfVectorizer(token_pattern=r'\\b[a-zA-Z0-9-][a-zA-Z0-9-]+\\b',min_df=50, max_df=.2)\n",
    "X_tfidf = tfidf.fit_transform(ngs_all)\n",
    "X_tfidf.shape\n",
    "\n",
    "tf_idf_array = X_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_array[90:100,-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "print(feature_names[:10])\n",
    "print(feature_names[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LDA with sklearn Cont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# create model with 20 topics\n",
    "lda = LatentDirichletAllocation(n_components=20,  # the number of topics\n",
    "                                n_jobs=-1,        # use all cpus\n",
    "                                random_state=123) # for reproducability\n",
    "\n",
    "# learn phi (lda.components_) and theta (X_lda)\n",
    "# this will take a while!\n",
    "X_lda = lda.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ngs_all[100][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_lda[100].round(2) # lda representation of document_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Note: since this is unsupervised, these numbers may change\n",
    "np.argsort(X_lda[100])[::-1]#[:3] # the top topics of document_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LDA: Per Topic Term Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# a utility function to print out the most likely terms for each topic\n",
    "# taken from https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic {:#2d}: \".format(topic_idx)\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print_top_words(lda,feature_names,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LDA Review\n",
    "\n",
    "- What did we learn?\n",
    "  - per document topic distributions\n",
    "  - per topic term distributions\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- What can we use this for?\n",
    "  - Dimensionality Reduction/Feature Extraction!\n",
    "  - investigate topics (much like PCA components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other NLP Features\n",
    "\n",
    "- Part of Speech tags\n",
    "- Dependency Parsing\n",
    "- Entity Detection\n",
    "- Word Vectors\n",
    "- See spaCy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using spaCy for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# uncomment the line below the first time you run this cell\n",
    "#%run -m spacy download en_core_web_sm\n",
    "try:\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "except OSError as e:\n",
    "    print('Need to run the following line in a new cell:')\n",
    "    print('%run -m spacy download en_core_web_sm')\n",
    "    print('or the following line from the commandline with eods-f20 activated:')\n",
    "    print('python -m spacy download en_core_web_sm')\n",
    "    \n",
    "parsed = nlp(\"N.Y.C. isn't in New Jersey.\")\n",
    "'|'.join([token.text for token in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy: Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $one billion.\")\n",
    "\n",
    "print(f\"{'text':7s} {'lemma':7s} {'pos':5s} {'is_stop'}\")\n",
    "print('-'*30)\n",
    "for token in doc:\n",
    "    print(f'{token.text:7s} {token.lemma_:7s} {token.pos_:5s} {token.is_stop}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy: Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy: Entity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "[(ent.text,ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy: Word Vectors\n",
    "\n",
    "- word2vec\n",
    "- shallow neural net\n",
    "- predict a word given the surrounding context (SkipGram or CBOW)\n",
    "- words used in similar context should have similar vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Need either the _md or _lg models to get vector information\n",
    "# Note: this takes a while!\n",
    "# %run -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md') # _lg has a larger vocabulary\n",
    "\n",
    "doc = nlp('Baseball is played on a diamond.')\n",
    "doc[0].text, doc[0].vector.shape, list(doc[0].vector[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy: Multiple Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use nlp.pipe to transform multiple docs at once\n",
    "docs = list(nlp.pipe(['Baseball is played on a diamond.',\n",
    "                      'Hockey is played on ice.',\n",
    "                      'Diamonds are clear as ice.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# using average of token vectors for each document.\n",
    "np.array([['{:.2f}'.format(docs[i].similarity(docs[j])) for j in range(3)]\n",
    "          for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning Sequences\n",
    "\n",
    "- Hidden Markov Models\n",
    "- Conditional Random Fields\n",
    "- Recurrant Neural Networks\n",
    "- LSTM\n",
    "- GPT3\n",
    "- [BERT](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)\n",
    "- Transformers (MLP 16.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Review\n",
    "\n",
    "- corpus, tokens, vocabulary, terms, n-grams, stopwords\n",
    "- tokenization\n",
    "- term frequency (TF), document frequency (DF)\n",
    "- TF vs TF-IDF\n",
    "- sentiment analysis\n",
    "- topic modeling\n",
    "<br>\n",
    "\n",
    "- POS\n",
    "- Dependency Parsing\n",
    "- Entity Extraction\n",
    "- Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# <center>Questions?</center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Appendix: LDA Plate Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/Smoothed_LDA.png\" width=\"400px\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<font size=5>\n",
    "    \n",
    "**K** :  number of topics\n",
    "\n",
    "**$\\varphi$** : per topic term distributions\n",
    "\n",
    "**$\\beta$**  : parameters for word distribution die factory, length = V (size of vocab)\n",
    "\n",
    "**M**     : number of documents\n",
    "\n",
    "**N**     : number of words/tokens in each document\n",
    "\n",
    "**$\\theta$** : per document topic distributions\n",
    "\n",
    "**$\\alpha$** : parameters for topic die factory, length = K (number of topics)\n",
    "\n",
    "**z** : topic indexes\n",
    "\n",
    "**w** : observed tokens\n",
    "\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "sigma2",
   "language": "python",
   "name": "sigma2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
