{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "### Due: Friday Dec. 1st @ 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will be performing \n",
    "\n",
    "- feature cleaning and engineering\n",
    "\n",
    "- dimensionality reduction with feature selection and extraction\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Replace Name and UNI in the first cell and filename\n",
    "- Follow the comments below and fill in the blanks (\\_\\_\\_\\_) to complete.\n",
    "- Where not specified, please run functions with default argument settings.\n",
    "- Please **'Restart and Run All'** prior to submission.\n",
    "- **Save pdf in Landscape** and **check that all of your code is shown** in the submission.\n",
    "- When submitting in Gradescope, be sure to **select which page corresponds to which question.**\n",
    "\n",
    "Out of 50 points total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. (2pts total) Homework Submission\n",
    "\n",
    "# (1pt) The homework should be spread over multiple pdf pages, not one single pdf page\n",
    "# (1pt) When submitting, assign each question to the pdf page where the solution is printed.\n",
    "#        If there is no print statement for a question, assign the question to the first pdf \n",
    "#        page where the code for the question is visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. (1pts) Set up our environment with comman libraries and plotting.\n",
    "#    Note: generally we would do all of our imports here but some imports\n",
    "#    have been left till later where they are used.\n",
    "\n",
    "# Import numpy, pandas, matplotlib.pyplot and seaborn with our usual aliases.\n",
    "____\n",
    "\n",
    "# Set the seaborn style to darkgrid\n",
    "____\n",
    "\n",
    "# Execute the matplotlib magic function to display plots inline\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Cleaning and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will be loading, cleaning and transforming a small set of data related to loan applications.\n",
    "\n",
    "There are two files, one containing loan application information and the other containing borrower information.\n",
    "\n",
    "You will need to load both files, join them and then transform this data, creating a new dataframe with features which could then be used for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. (1pts) Load Loan Application Data\n",
    "\n",
    "# Read in the first dataframe containing loan application information.\n",
    "#  The path to the datafile is '../data/hw3_loan.csv'.\n",
    "#  Use the appropriate pandas command to read a csv file with default arguments.\n",
    "# Store this dataframe as df_loan.\n",
    "____\n",
    "\n",
    "# Assert that the data is the correct shape\n",
    "assert df_loan.shape == (664,4)\n",
    "\n",
    "# Print the output of .info() called on df_loan \n",
    "#  Note that 2 columns have missing values\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. (2pts) Deal with Duplicates\n",
    "\n",
    "# Display rows with duplicate CustomerIDs\n",
    "#  remember to use subset= to set the column of interest\n",
    "#  use keep=False to show all duplicates\n",
    "# We should see a DataFrame with 2 rows\n",
    "____\n",
    "\n",
    "# Drop one of the rows with duplicate CustomerID,\n",
    "#   keeping the first duplicate row (default)\n",
    "# Store into df_loan_nodups\n",
    "____\n",
    "\n",
    "# We should only drop one row\n",
    "assert df_loan_nodups.shape == (663,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. (1pts) Set the Index of df_loan_nodups\n",
    "\n",
    "# Set the index of df_loan_nodups to the CustomerID column to make joining easier\n",
    "#  use .set_index()\n",
    "#  drop the original index\n",
    "# Store back into df_loan_nodups\n",
    "____\n",
    "\n",
    "# Display the first 3 rows of df_loan_nodups to visually confirm that the index has been set\n",
    "#  You should see 3 rows and 3 columns\n",
    "# Note that that the index CustomerID starts at 2 instead of 0\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. (1pts) Load Borrower Data\n",
    "\n",
    "# Read in a second table containing borrower information.\n",
    "#  The path to the datafile is '../data/hw3_borrower.csv'.\n",
    "#  Use the appropriate pandas command to read a csv file.\n",
    "#  IMPORTANT: set the index as the 'CustomerID' column using the index_col= argument.\n",
    "#  Store this dataframe as df_borrower.\n",
    "____\n",
    "\n",
    "# Assert that the data is the correct shape\n",
    "assert df_borrower.shape == (663,1)\n",
    "\n",
    "# Print the output of .info() called on df_borrower\n",
    "#  Note that the index has been set and there are no missing values\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. (2pts) Join Datasets\n",
    "\n",
    "# Join the df_loan_nodups and df_borrower datasets\n",
    "# Perform a left join, with df_loan_nodups as the \"left\" table \n",
    "#    and df_borrower as the \"right\".\n",
    "# Since the dataframes share an index (CustomerID), it is convenient \n",
    "#    to use the .join() function instead of .merge().\n",
    "# Store the resulting dataframe as df\n",
    "____\n",
    "\n",
    "# Assert that the data is the correct shape\n",
    "assert df.shape == (663,4)\n",
    "\n",
    "# Print the output of .info() called on df\n",
    "# There should still be 663 rows but now 4 columns, 2 with missing values\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. (1pts) Create df_features\n",
    "\n",
    "# We'll perform the transformations below to get the data ready for modeling.\n",
    "#\n",
    "# Instead of adding transformed features into our original dataframe (df)\n",
    "#   it is convenient to create a new dataframe containing only features.\n",
    "# This will eventually be the X features for our models.\n",
    "\n",
    "# Create a new, empty, DataFrame called df_features\n",
    "#   that has the same index as df (index=df.index)\n",
    "____\n",
    "\n",
    "# Print the output of .info() called on df_features\n",
    "# The index should match the index of df above, but empty otherwise\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RequestedAmount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. (1pts) Plot RequestedAmount\n",
    "\n",
    "# RequestedAmount is a numeric feature with missing values\n",
    "\n",
    "# Use seaborn histplot to plot df.RequestedAmount using default settings.\n",
    "# Note that this feature is right skewed and has a wide range.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. (2pts) Create Dummy Column for Missing RequestedAmount \n",
    "\n",
    "# Before filling the missing values we should create a dummy variable\n",
    "#   to capture which rows had missing values\n",
    "\n",
    "# Find the rows where RequestedAmount is missing\n",
    "#  and convert the resulting boolean values to integers\n",
    "# Store in df_features as 'RequestedAmount_missing'.\n",
    "____\n",
    "\n",
    "# Print the number of 0s and 1s in the RequestedAmount_missing column using .value_counts().\n",
    "#   (There should be 12 1s meaning that there are 12 missing values)\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. (2pts) Fill Missing Values in RequestedAmount\n",
    "\n",
    "# As RequestedAmount is right skewed, we'll fill missing values using median instead of mean.\n",
    "\n",
    "# Print the median of RequestedAmount before filling\n",
    "print(f'RequestedAmount median        : {____}')\n",
    "\n",
    "# Use fillna() to fill the missing values in RequestedAmount \n",
    "#   with the median of RequestedAmount\n",
    "# We'll make two more transformations to this column before storing it as a feature\n",
    "#   so store back into df as 'RequestedAmount_filled'\n",
    "____\n",
    "\n",
    "# Print the median of RequestedAmount_filled\n",
    "#  The median should not have changed after filling\n",
    "print(f'RequestedAmount_filled median : {____}')\n",
    "\n",
    "# Assert that there are no longer any missing values in the RequestedAmount_filled column of df\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. (2pts) Log Transform RequestedAmount\n",
    "\n",
    "# Using .apply(), apply np.log (without parentheses) to the RequestedAmount_filled column.\n",
    "# Store the result back into df as RequestedAmount_log\n",
    "____\n",
    "\n",
    "# Use seaborn histplot() (using default settings) to plot RequestedAmount_log \n",
    "# Note that the shape is now closer to a normal distribution\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. (3pts) Center and Scale RequestedAmount_log Using StandardScaler\n",
    "\n",
    "# Import StandardScaler from sklearn\n",
    "____\n",
    "\n",
    "# Using StandardScaler (with default settings) \n",
    "#   run fit_transform() to standardize RequestedAmount_log\n",
    "# Note that fit_transform expects a DataFrame not a Series \n",
    "#   Recall: we can get a DataFrame containing one column by indexing using a list\n",
    "# Store the result in df_features as 'RequestedAmount_logscaled'\n",
    "____\n",
    "\n",
    "# Confirm that scaling has been applied properly by printing out \n",
    "#    the 'mean' and 'std' of df_features.RequestedAmount_logscaled\n",
    "#    using the .agg() function \n",
    "#    rounded to a precision of 2\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoanReason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. (1pts) LoanReason Values\n",
    "\n",
    "# df.LoanReason is a categorical variable.\n",
    "\n",
    "# Print the frequency counts of each category, including missing values\n",
    "#   using .value_counts() with dropna=False\n",
    "# (You should see a row for NaN indicating 23 missing values)\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. (2pts) Fill Missing Values in LoanReason and Store in df_features\n",
    "\n",
    "# Since this is a categorical variable, instead of creating a \"missing\" dummy column\n",
    "#    we'll simply fill the missing values with the string 'MISSING'\n",
    "\n",
    "# Fill the missing values of LoanReason with the string 'MISSING'\n",
    "# Store into df_features as LoanReason\n",
    "____\n",
    "\n",
    "# Print the number of items in each category in df_features.LoanReason, including nan's\n",
    "#   using value_counts() with dropna=False\n",
    "# (You should see a row for MISSING but no row for NaN)\n",
    "____\n",
    "\n",
    "# We'll deal with One-Hot Encoding LoanReason after dealing with Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. (2pts) Scale and Store Ages\n",
    "\n",
    "# The last variable we'll deal with the numeric variable Age.\n",
    "\n",
    "# Assert that df.Age doesn't have any missing values\n",
    "____\n",
    "\n",
    "# Print the min and max values for df.Age using .agg()\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. (1pts) Create Age Bin Edges for Age\n",
    "\n",
    "# We'll transform Age into a categorical variable using binning.\n",
    "# Note that this is for practice and there aren't any clear indications\n",
    "#    in the data that we should be binning this way.\n",
    "\n",
    "# We'll bin Age into 3 three equal sized groups\n",
    "# To get the bin edges use the Series .quantile() method\n",
    "# The quantiles we want are q=[0,.33,.66,1]\n",
    "# Store the bin edges as age_bins\n",
    "____\n",
    "\n",
    "# Print the bin edges\n",
    "# Rows labeled 0.00 and 1.00 should have values that match the Age min and max seen printed above\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. (2pts) Bin Age\n",
    "\n",
    "# Use pd.cut() to bin Age\n",
    "#  Use the age_bins list we created above for the bin edges.\n",
    "#  Set right=True to include right edge in each bin.\n",
    "#  Set include_lowest=True to include the minimum value in the first bin.\n",
    "#  All other arguments as their default.\n",
    "# Store in df_features as Age\n",
    "____\n",
    "\n",
    "# Print the first 3 rows of df_features.Age\n",
    "# By default, the label names are the bin edges\n",
    "____\n",
    "\n",
    "print() # print a blank line\n",
    "\n",
    "# Also, print the first 3 rows of df.Age to visually confirm the correct bins have been applied\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. (3pts) Transform LoanReason and Age Bins using One-Hot Encoding\n",
    "\n",
    "# Once we One-Hot Encode our features, the number of columns can increase dramitically.\n",
    "# For DataFrames with many columns it is helpful to display the transpose of a subset of rows.\n",
    "\n",
    "# Display the first 3 rows of df_features\n",
    "#  rounded to a precision of 2\n",
    "#  transposed using .transpose() or .T\n",
    "# Should see 4 rows, 3 columns\n",
    "____\n",
    "\n",
    "# Use pd.get_dummies() to encode the categorical variables\n",
    "#  Pass the entire df_features DataFrame\n",
    "#  Note: pd.get_dummies() will encode any columns with dtype `object` or `category`\n",
    "# Store as df_features_ohe\n",
    "____\n",
    "\n",
    "# Display the first 3 rows of df_features_ohe rounded to a precision of 2 transposed\n",
    "# Now we should see 10 rows and 3 columns\n",
    "# Note that all features are numeric and the One-Hot Encoding has been applied\n",
    "____\n",
    "\n",
    "# Assert that df_features_ohe now has 663 rows and 10 columns\n",
    "assert df_features_ohe.shape == (663,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. (2pts) Transform Target \n",
    "\n",
    "# The target we're interested in predicting is df.WasTheLoanApproved.\n",
    "# This is a categorical variable taking the values Y for yes and N for no\n",
    "\n",
    "# Transform the target df.WasTheLoanApproved\n",
    "#    into integers where Y maps to 1 and N maps to 0 using .map()\n",
    "# Recall .map() takes a dictionary of key:value pairs where\n",
    "#   keys   = what you want to map from\n",
    "#   values = what you want to map to\n",
    "# Store the resulting Series in y\n",
    "____\n",
    "\n",
    "# Print the proportion of positives (1's) in y with a precision of 2\n",
    "#  Note that there are more 1's than 0's\n",
    "#  We can use this as our baseline accuracy (what would be found by a DummyClassifier)\n",
    "#  We'd like to find a model that does better than this\n",
    "print(f'proportion of positives in y: {____}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21. (1pts) Split the Data\n",
    "\n",
    "# Before we continue we should split up our data into a train and test set\n",
    "\n",
    "# import train_test_split from sklearn\n",
    "____\n",
    "\n",
    "# Generate a training and test set from df_features_ohe and y\n",
    "#   with test_size of 10% of the data\n",
    "#   stratified by y\n",
    "#   and random_state=512\n",
    "# Store in X_train,X_test,y_train,y_test\n",
    "____\n",
    "\n",
    "# Assert that X_train has 596 rows, 10 columns\n",
    "assert X_train.shape == (596,10)\n",
    "\n",
    "# Print the proportion of 1s in y_test rounded to a precision of 2\n",
    "#  to visually confirm that the proportion is close to that seen in y (plus or minus .01)\n",
    "print(f'proportion of positives in y_test: {____}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#22. (3pts) Rank Feature Importance Using GradientBoostingClassifier\n",
    "\n",
    "# Import GradientBoostingClassifier from sklearn\n",
    "____\n",
    "\n",
    "# Instantiate a GradientBoostingClassifier object\n",
    "#  with n_estimators=10, \n",
    "#  max_depth=5,\n",
    "#  and all other arguments as their default.\n",
    "# Store as gbc\n",
    "____\n",
    "\n",
    "# Fit gbc on the training set\n",
    "____\n",
    "\n",
    "# The feature_importances_ stored in gbc are in the order of the columns of X_train\n",
    "# Create a new Series \n",
    "#    with values from gbc.feature_importances_\n",
    "#    with the index set to the columns of X_train\n",
    "# Store in gbc_feature_importances\n",
    "____\n",
    "\n",
    "# Display feature_importances sorted by value descending rounded to a precision of 2\n",
    "# Note that the most informative feature should be RequestedAmount_logscaled\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23. (3pts) Feature Selection with SelectFromModel\n",
    "\n",
    "# Import SelectFromModel from sklearn\n",
    "____\n",
    "\n",
    "# Instantiate a SelectFromModel transformer with\n",
    "#   gbc as the estimator \n",
    "#   threshold='mean' (the default)\n",
    "#   prefit=False (the default)\n",
    "#   fit on X_train,y_train to avoid a warning about missing feature_names below\n",
    "# Store as sfm\n",
    "____\n",
    "\n",
    "# Show the selected features using X_train.columns and sfm.get_support()\n",
    "# Recall that sfm.get_support() returns a boolean mask over the features\n",
    "#   with a value of True where the feature has been selected\n",
    "# The features shown should be the top 2 features listed in the previous cell\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24. (2pts) Transform Data Using Selected Features\n",
    "\n",
    "# Create a new dataset using only the features selected in the previous step.\n",
    "# Use sfm to transform X_train and store as X_train_fs\n",
    "____\n",
    "\n",
    "# Use sfm to transform X_test and store as X_test_fs\n",
    "____\n",
    "\n",
    "# Assert that X_train_fs has 596 rows and 2 columns.\n",
    "assert X_train_fs.shape == (596,2)\n",
    "\n",
    "# Print the first 3 rows of X_train_fs, rounded to a precision of 2\n",
    "# Note that this will be a numpy array and not a DataFrame\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25. (2pts) Train and Evaluate Model On Selected Features\n",
    "\n",
    "# Instantiate a new GradientBoostingClassifier\n",
    "#   with n_estimators=10, \n",
    "#   max_depth=5,\n",
    "#   and all other parameters as the default\n",
    "# Store in gbc_fs\n",
    "____\n",
    "\n",
    "# Train the gbc_fs model on X_train_fs and y_train\n",
    "____\n",
    "\n",
    "# Print the accuracy achieved by gbc_fs on both \n",
    "#   the training (X_train_fs,y_train) and test set (X_test_fs,y_test) \n",
    "#   with precision of 2 decimal places in both cases\n",
    "# On both we should do better than the baseline accuracy calculated above\n",
    "print(f'training accuracy: {____}')\n",
    "print(f'test accuracy    : {____}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26. (3pts) Reduce Dataset to 2D Using PCA\n",
    "\n",
    "# Import PCA from sklearn\n",
    "____\n",
    "\n",
    "# Instantiate a pca object with\n",
    "#   n_components=2\n",
    "#   random_state=512\n",
    "# Store as pca\n",
    "____\n",
    "\n",
    "# Fit and transform the full X_train to 2D using pca\n",
    "# Store in X_train_pca\n",
    "____\n",
    "\n",
    "# Transform (but don't fit!) the X_test to 2D using the trained pca\n",
    "# Store in X_test_pca\n",
    "___\n",
    "\n",
    "# Print the ratio of variance explained by each component in pca, rounded to a precision of 2\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27. (2pts) Train and evaluate a classifier using the PCA representation\n",
    "\n",
    "# Train a new GradientBoostingClassifier\n",
    "#  with n_estimators=10\n",
    "#  and max_depth=5\n",
    "#  on X_train_pca, y_train\n",
    "# Store as gbc_pca\n",
    "____\n",
    "\n",
    "# Print the accuracy achieved by gbc_pca on both \n",
    "#   the training (X_train_pca,y_train) and test set (X_test_pca,y_test) \n",
    "#   with precision of 2 decimal places in both cases\n",
    "# Note that, while the gbc_pca model is not performing quite as well as gbc_fs,\n",
    "#   the first 2 components of the PCA representation are only explaining 60% of the variation in the dataset\n",
    "print(f'training accuracy: {____}')\n",
    "print(f'test accuracy    : {____}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eods-f22",
   "language": "python",
   "name": "eods-f22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
